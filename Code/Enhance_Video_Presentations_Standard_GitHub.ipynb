{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5V2T4L2inBE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0td6bZcEZRda"
      },
      "outputs": [],
      "source": [
        "!pip install pydub SpeechRecognition\n",
        "!pip install requests openai\n",
        "!pip install moviepy ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3MpZM-WrPjdH"
      },
      "outputs": [],
      "source": [
        "# Install the Deepgram Python SDK\n",
        "# https://github.com/deepgram/deepgram-python-sdk\n",
        "\n",
        "!pip install deepgram-sdk\n",
        "\n",
        "# Install python-dotenv to protect API key\n",
        "\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsXCG0KFVqS2"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "from dotenv import load_dotenv\n",
        "from deepgram import DeepgramClient, PrerecordedOptions, FileSource\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import date, datetime\n",
        "import requests\n",
        "import textwrap\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import re\n",
        "import openai\n",
        "import cvxpy as cp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqqtDvgQUnpu"
      },
      "outputs": [],
      "source": [
        "base_directory = '/content/drive/MyDrive/'\n",
        "video_sample = 'S1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09dHLA5ndEjl"
      },
      "outputs": [],
      "source": [
        "playHT_API_USER = \"?\"\n",
        "playHT_API_KEY = \"?\"\n",
        "\n",
        "openai_api_key = \"?\"\n",
        "\n",
        "Deepgram_user = \"?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxenZ3r_TqUU"
      },
      "source": [
        "---\n",
        "================================================\n",
        "# Pipeline\n",
        "================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzluy-Fc0ROB"
      },
      "source": [
        "# 1. Extract audio from the video\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgEDK94L0aEd"
      },
      "outputs": [],
      "source": [
        "video_filename = f\"/content/drive/MyDrive/{video_sample}_original.mp4\"\n",
        "# Load the video file\n",
        "video_clip = VideoFileClip(video_filename)\n",
        "\n",
        "audio_clip = video_clip.audio\n",
        "audio_clip.write_audiofile(f\"{base_directory}{video_sample}.wav\")\n",
        "\n",
        "# Close the clips to release resources\n",
        "video_clip.close()\n",
        "audio_clip.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXPNrCgDAgIK"
      },
      "source": [
        "# 2. Voice clone\n",
        "\n",
        "* Upload the original audio to playht to generate a voice id for future voice clone (test to speech)\n",
        "* Do not need for every time, suggest to put this to first step\n",
        "\n",
        "  https://play.ht/studio/api-access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXXZgQnDdlwE"
      },
      "outputs": [],
      "source": [
        "# create voice\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://api.play.ht/api/v2/cloned-voices/instant\"\n",
        "\n",
        "files = { \"sample_file\": (f\"{base_directory}{video_sample}.wav\", open(f\"{base_directory}{video_sample}.wav\", \"rb\"), \"audio/wav\") }\n",
        "payload = { \"voice_name\": f'{video_sample}' }\n",
        "headers = {\n",
        "    \"accept\": \"application/json\",\n",
        "    \"AUTHORIZATION\": f'{playHT_API_KEY}',\n",
        "    \"X-USER-ID\": f'{playHT_API_USER}'\n",
        "}\n",
        "\n",
        "response = requests.post(url, data=payload, files=files, headers=headers)\n",
        "print(response.text)\n",
        "voice_url = json.loads(response.text)['id']\n",
        "print(voice_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtnEy6Pl0R84"
      },
      "source": [
        "# 3. audio cut of original audio if neccessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnS5HFnIRqhJ"
      },
      "outputs": [],
      "source": [
        "# # Load original audio\n",
        "# audio = AudioSegment.from_file(f\"{base_directory}{video_sample}.wav\")\n",
        "\n",
        "# # cutpoint\n",
        "# cut_point1 = 60000  #60000 as 1 minute\n",
        "# cut_point2 = 314000  #314000 as 5:14\n",
        "\n",
        "# # cut audio\n",
        "# part_temp1 = audio[:cut_point1]\n",
        "# part_temp2 = audio[:cut_point2]\n",
        "\n",
        "# # export\n",
        "# part_temp1.export(f\"{base_directory}{video_sample}_1_min.wav\", format=\"wav\")\n",
        "# part_temp2.export(f\"{base_directory}{video_sample}_5_min.wav\", format=\"wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5LuEkinVEtm"
      },
      "source": [
        "# 4. Audio to text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtRdGumBVPMz"
      },
      "source": [
        " ---\n",
        "\n",
        "  * Deepgram Nova-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYiTCLaKVdOy"
      },
      "source": [
        "  ---\n",
        "  * Whisper via Deepgram\n",
        "    - Model selection: https://developers.deepgram.com/docs/model\n",
        "    - Speech2text: https://developers.deepgram.com/docs/getting-started-with-pre-recorded-audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C71007-sx-f1"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from deepgram import DeepgramClient, PrerecordedOptions, FileSource\n",
        "\n",
        "load_dotenv()\n",
        "AUDIO_FILE = f\"{base_directory}{video_sample}.wav\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        deepgram = DeepgramClient(f\"{Deepgram_user}\")\n",
        "\n",
        "        with open(AUDIO_FILE, \"rb\") as file:\n",
        "            buffer_data = file.read()\n",
        "\n",
        "        payload: FileSource = {\n",
        "            \"buffer\": buffer_data,\n",
        "        }\n",
        "\n",
        "        options = PrerecordedOptions(\n",
        "            model=\"nova-2\", #nova-2 #whisper-large\n",
        "            smart_format=True,\n",
        "            punctuate=True,\n",
        "            diarize=True,\n",
        "            utt_split= 0.8\n",
        "\n",
        "        )\n",
        "\n",
        "        response = deepgram.listen.rest.v(\"1\").transcribe_file(payload, options)\n",
        "\n",
        "        # Convert response to JSON format\n",
        "        response_json = response.to_json(indent=4)\n",
        "\n",
        "        # Print the JSON output to the console\n",
        "        print(response_json)\n",
        "\n",
        "        # Save the JSON output to a file\n",
        "        with open(f\"{base_directory}transcription_output_nova_2_{video_sample}.json\", 'w') as json_file:\n",
        "            json_file.write(response_json)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NmRn_X56xUa"
      },
      "source": [
        "---\n",
        "# 5. Original audio timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3aSRWFob52q"
      },
      "source": [
        "---\n",
        "script and timestamp from DeepGram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtE2X-4_dAlB"
      },
      "source": [
        "## $\\color{purple}{\\text {This is slicing based on sentence}}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwDT5z9vxJp7"
      },
      "outputs": [],
      "source": [
        "# Factors setting\n",
        "\n",
        "speed_factor = 2.62  # Scaling factor used in the speed calculation for playHT speed rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bpSQdZBLq65N"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    with open(f\"{base_directory}transcription_output_nova_2_{video_sample}.json\", 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    full_script = \"\"\n",
        "    segment_dictionary = {}\n",
        "    segment_durations = {}\n",
        "    pauses_between_segments = {}\n",
        "    segment_speed_dictionary = {}\n",
        "    first_segment_start_time = None\n",
        "    last_segment_end_time = None\n",
        "    total_duration = 0\n",
        "    segment_length = 0\n",
        "    segment_counter = 1\n",
        "    pause_counter = 1\n",
        "    last_end = 0\n",
        "\n",
        "    channels = data['results']['channels']\n",
        "\n",
        "    for channel in channels:\n",
        "        words = channel['alternatives'][0]['words']\n",
        "        current_words = []\n",
        "        segment_start_time = None\n",
        "\n",
        "        for i, word_info in enumerate(words):\n",
        "            current_word = word_info['punctuated_word']\n",
        "            word_start = word_info['start']\n",
        "            word_end = word_info['end']\n",
        "\n",
        "            if segment_start_time is None:\n",
        "                segment_start_time = word_start  # Initialize segment start time\n",
        "\n",
        "            current_words.append(current_word)\n",
        "\n",
        "            # Calculate word gap\n",
        "            if i + 1 < len(words):\n",
        "                next_word_start = words[i + 1]['start']\n",
        "                word_gap = next_word_start - word_end\n",
        "            else:\n",
        "                word_gap = 0  # Last word in the list\n",
        "\n",
        "            # Determine if the current segment should end\n",
        "            should_end_segment = False\n",
        "\n",
        "            if current_word.endswith(('!', '?', '.', '。', '！', '？')):\n",
        "                should_end_segment = True\n",
        "\n",
        "            if should_end_segment:\n",
        "                current_segment = \" \".join(current_words)\n",
        "                current_words = []  # Reset for the next segment\n",
        "\n",
        "                if first_segment_start_time is None:\n",
        "                    first_segment_start_time = segment_start_time\n",
        "\n",
        "                # Update script and segment details\n",
        "                full_script += current_segment + \" \"\n",
        "                segment_duration = word_end - segment_start_time\n",
        "                segment_durations[f\"S{segment_counter}\"] = segment_duration\n",
        "                segment_dictionary[f\"S{segment_counter}\"] = current_segment\n",
        "                total_duration += segment_duration\n",
        "\n",
        "                # Calculate segment speed\n",
        "                segment_length = len(current_segment.rstrip().split())\n",
        "                segment_speed = segment_length / (speed_factor * segment_duration)\n",
        "                segment_speed_dictionary[f\"S{segment_counter}\"] = segment_speed\n",
        "\n",
        "                # Calculate pause between segments\n",
        "                if last_end > 0 and segment_start_time >= last_end:\n",
        "                    pause_duration = segment_start_time - last_end\n",
        "                    pauses_between_segments[f\"P{pause_counter}\"] = pause_duration\n",
        "                    pause_counter += 1\n",
        "\n",
        "                segment_counter += 1\n",
        "                segment_start_time = None  # Reset for the next segment\n",
        "                last_end = word_end\n",
        "\n",
        "        # Process any remaining words as the last segment\n",
        "        if current_words:\n",
        "            current_segment = \" \".join(current_words)\n",
        "\n",
        "            if first_segment_start_time is None:\n",
        "                first_segment_start_time = segment_start_time\n",
        "\n",
        "            full_script += current_segment + \" \"\n",
        "            segment_duration = word_end - segment_start_time\n",
        "            segment_durations[f\"S{segment_counter}\"] = segment_duration\n",
        "            segment_dictionary[f\"S{segment_counter}\"] = current_segment\n",
        "            total_duration += segment_duration\n",
        "\n",
        "            segment_length = len(current_segment.rstrip().split())\n",
        "            segment_speed = segment_length / (speed_factor * segment_duration)\n",
        "            segment_speed_dictionary[f\"S{segment_counter}\"] = segment_speed\n",
        "\n",
        "            if last_end > 0 and segment_start_time >= last_end:\n",
        "                pause_duration = segment_start_time - last_end\n",
        "                pauses_between_segments[f\"P{pause_counter}\"] = pause_duration\n",
        "                pause_counter += 1\n",
        "\n",
        "            segment_counter += 1\n",
        "            last_end = word_end\n",
        "\n",
        "    word_count = len(full_script.rstrip().split())\n",
        "    last_segment_end_time = last_end\n",
        "\n",
        "    # Calculate estimated speech rate\n",
        "    if total_duration > 0:\n",
        "        estimate_speed = word_count / (speed_factor * total_duration)\n",
        "    else:\n",
        "        estimate_speed = 0  # Prevent division by zero\n",
        "\n",
        "    return full_script, segment_dictionary, segment_durations, segment_speed_dictionary, pauses_between_segments, first_segment_start_time, last_segment_end_time, total_duration, word_count, estimate_speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhyR-GkuxP5I"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    results = main()\n",
        "    full_script, segment_dictionary, segment_durations, segment_speed_dictionary, pauses_between_segments, first_segment_start_time, last_segment_end_time, total_duration, word_count, estimate_speed = results\n",
        "\n",
        "    # print(\"Refined Script:\")\n",
        "    # print(full_script)\n",
        "    print(\"\\nSegment Content:\")\n",
        "    for key, value in segment_dictionary.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print(\"\\nSegment Durations (seconds):\")\n",
        "    for key, value in segment_durations.items():\n",
        "        print(f\"{key}: {value:.2f}\")\n",
        "    # print(\"\\nSegment Speed:\")\n",
        "    # for key, value in segment_speed_dictionary.items():\n",
        "    #     print(f\"{key}: {value:.2f}\")\n",
        "    print(\"\\nPauses Between Segments (seconds):\")\n",
        "    for key, value in pauses_between_segments.items():\n",
        "        print(f\"{key}: {value:.2f}\")\n",
        "    print(f\"\\nStart time of the first segment:\\n{first_segment_start_time} seconds\")\n",
        "    print(f\"\\nEnd time of the last segment:\\n{last_segment_end_time} seconds\")\n",
        "    print(f\"\\nTotal duration of all segments:\\n{total_duration:.2f} seconds\")\n",
        "    print(f\"\\nTotal word count of the script:\\n{word_count} words\")\n",
        "    print(f\"\\nEstimated Speech Speed:\\n{estimate_speed:.2f} words/({speed_factor} * second)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svWrTmEgVEMG"
      },
      "outputs": [],
      "source": [
        "adjust_speed = 0\n",
        "if estimate_speed < 0.8:\n",
        "   adjust_speed = 0.8\n",
        "elif estimate_speed < 1.0 and estimate_speed >= 0.8:\n",
        "   adjust_speed = round(estimate_speed,2)\n",
        "elif estimate_speed >= 1.0:\n",
        "   adjust_speed = 1.0\n",
        "\n",
        "adjust_speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLN01MHjxWlr"
      },
      "outputs": [],
      "source": [
        "segment_adjust_word_count_dictionary = segment_durations.copy()\n",
        "\n",
        "for key in segment_adjust_word_count_dictionary:\n",
        "    segment_content = segment_dictionary[key]\n",
        "\n",
        "    original_length = len(segment_content.split())\n",
        "    polished_length = round(speed_factor * adjust_speed * segment_durations[key])\n",
        "\n",
        "    # print(key, original_length, polished_length)\n",
        "\n",
        "    # Apply the condition to adjust the word count\n",
        "    if abs(original_length - polished_length) < 3 or original_length < 6:\n",
        "        segment_adjust_word_count_dictionary[key] = original_length\n",
        "    else:\n",
        "        segment_adjust_word_count_dictionary[key] = polished_length\n",
        "        print(key,\" -- now:original \",segment_adjust_word_count_dictionary[key],\" \", original_length)\n",
        "\n",
        "\n",
        "print(\"Adjusted Segment Word Counts:\")\n",
        "print(segment_adjust_word_count_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R2jE9MNtmdbe"
      },
      "outputs": [],
      "source": [
        "segment_adjust_word_count_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWZvoEE_dJ_Q"
      },
      "outputs": [],
      "source": [
        "segment_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmXrnDqu8MGc"
      },
      "source": [
        "---\n",
        "# 6. OpenAI prompt  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fF12qHo6ylP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXl4g3ae61PE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhI_wTASb5K8"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=f'{openai_api_key}')\n",
        "\n",
        "def chat(prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",#gpt-3.5-turbo\t#gpt-4o #gpt-4\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "# Define the prompt\n",
        "prompt = '''Please update the `segment_dictionary` using the complete transcript, following these criteria, especially No. 2 criteria:\n",
        "\n",
        "1. Correct grammatical errors, proper nouns that may have been mispronounced in each segment.\n",
        "2. Adjust each segment legnth to meet the exact word count number specified in the `segment_adjust_word_count_dictionary` **(important)**.\n",
        "3. Retain the original total number of segments.\n",
        "4. Ensure each revised segment corresponds exactly to its original; do not mix content between segments.\n",
        "5. Make the script make sense as a whole, segments should form a coherent script, while polishing each segment individually.\n",
        "6. Minimize changes to the original text.\n",
        "7. Exam again to ensure that each segment legnth to meet the exact word count number specified in the `segment_adjust_word_count_dictionary` **(important)**.\n",
        "8. Format the output as a Python dictionary using double quotes (`\"`) for keys and values.\n",
        "9. Only output the updated dictionary.'''\n",
        "\n",
        "\n",
        "revised_text = chat(f\"{prompt}\\n\\nsegment_dictionary: {segment_dictionary}\\n\\nsegment_adjust_word_count_dictionary: {segment_adjust_word_count_dictionary}\")\n",
        "clean_string = revised_text.replace(\"```python\\n\", \"\").replace(\"```\", \"\").replace(\"\\n    \", \"\").replace(\"\\n\", \"\").strip()\n",
        "\n",
        "try:\n",
        "    revised_segment_dictionary = json.loads(clean_string)\n",
        "    print(revised_segment_dictionary)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(\"Error parsing JSON:\", e)\n",
        "    print(\"Received string:\", clean_string)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZJbF5hAg5lx"
      },
      "source": [
        "#  !!! Compare the revised script with the original one !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf_4tBo0L1oB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIVQnsb6DKvL"
      },
      "outputs": [],
      "source": [
        "original_word_count = sum(len(value.split()) for value in segment_dictionary.values())\n",
        "print(\"Original_word_count:\", original_word_count)\n",
        "\n",
        "punctuation_marks = \".?!\"\n",
        "sentence_count = sum(len(re.findall(f\"[{re.escape(punctuation_marks)}]\", value)) for value in segment_dictionary.values())\n",
        "print(\"Total original sentence counts:\", sentence_count)\n",
        "\n",
        "revised_word_count = sum(len(value.split()) for value in revised_segment_dictionary.values())\n",
        "print(\"Revised_word_count:\", revised_word_count)\n",
        "\n",
        "punctuation_marks = \".?!\"\n",
        "sentence_count = sum(len(re.findall(f\"[{re.escape(punctuation_marks)}]\", value)) for value in revised_segment_dictionary.values())\n",
        "print(\"Total revised sentence counts:\", sentence_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHHh_aItL1VA"
      },
      "outputs": [],
      "source": [
        "for key in segment_dictionary:\n",
        "    if key in revised_segment_dictionary:\n",
        "        original_len = len(segment_dictionary[key].split())\n",
        "        revised_len = len(revised_segment_dictionary[key].split())\n",
        "\n",
        "\n",
        "        if original_len != revised_len:\n",
        "            print(f\"Length of '{key}' in original: {original_len}\")\n",
        "            print(f\"Length of '{key}' in revised: {revised_len}\")\n",
        "            print(f\"\\nThe original segment:{segment_dictionary[key]}\\nThe revised segment :{revised_segment_dictionary[key]}\\n\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Key '{key}' not found in revised_segment_dictionary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GMWyinozi9_6"
      },
      "outputs": [],
      "source": [
        "for key, value in revised_segment_dictionary.items():\n",
        "    if value[0].islower():\n",
        "        revised_segment_dictionary[key] = ' ' + value[0].upper() + value[1:]\n",
        "\n",
        "    value = revised_segment_dictionary[key].strip()\n",
        "    if value.endswith(','):\n",
        "        revised_segment_dictionary[key] = value[:-1] + '.'\n",
        "    elif not value.endswith('.') and not value.endswith('!') and not value.endswith('?') and not value.endswith('...'):\n",
        "        revised_segment_dictionary[key] = value + '.'\n",
        "revised_segment_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bARx95flPx0q"
      },
      "outputs": [],
      "source": [
        "date = date.today().strftime(\"_%Y%m%d\")\n",
        "filename = f'{base_directory}{video_sample}_revised_segment_dictionary{date}.json'\n",
        "\n",
        "with open(filename, 'w') as file:\n",
        "    json.dump(revised_segment_dictionary, file, indent=4)\n",
        "\n",
        "# print(f'Dictionary saved as {filename}')\n",
        "\n",
        "filename = f'{base_directory}{video_sample}_segment_dictionary{date}.json'\n",
        "\n",
        "with open(filename, 'w') as file:\n",
        "    json.dump(segment_dictionary, file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih3wWePI7jvh"
      },
      "outputs": [],
      "source": [
        "with open(f'{base_directory}{video_sample}_revised_segment_dictionary{date}.json', 'r') as file:\n",
        "    revised_segment_dictionary = json.load(file)\n",
        "    print(revised_segment_dictionary)\n",
        "\n",
        "with open(f'{base_directory}{video_sample}_segment_dictionary{date}.json', 'r') as file:\n",
        "    segment_dictionary = json.load(file)\n",
        "    print(segment_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlEt1FtpPUh7"
      },
      "outputs": [],
      "source": [
        "revised_script = \"\"\n",
        "\n",
        "for key in revised_segment_dictionary:\n",
        "    revised_script += revised_segment_dictionary[key] + \" \"\n",
        "print(revised_script)\n",
        "len(revised_script),len(full_script)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qDW9wOAAoWa"
      },
      "source": [
        "# 7. TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4zh15CyZGmQ"
      },
      "outputs": [],
      "source": [
        "adjust_speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14IaSAMYWX6T"
      },
      "outputs": [],
      "source": [
        "# TTS API endpoint\n",
        "url = \"https://api.play.ht/api/v2/tts\"\n",
        "\n",
        "# Open a file to save the responses\n",
        "with open(f\"{base_directory}tts_responses_{video_sample}_adj_speed_{adjust_speed:.1f}.txt\", 'w') as file:\n",
        "    # Loop through each segment and send it to the TTS service\n",
        "    for key, value in revised_segment_dictionary.items():\n",
        "        content = value\n",
        "\n",
        "        if content and not content.endswith(('.', ',', '?', '!', '...')):\n",
        "            content += '.'  # Ensure each segment ends with a period if not already punctuated\n",
        "        payload = {\n",
        "            \"text\": content,\n",
        "            \"voice\": f\"{voice_url}\",\n",
        "            \"output_format\": \"wav\",\n",
        "            \"voice_engine\": \"PlayHT2.0\",\n",
        "            \"voice_guidance\": 1,\n",
        "            \"quality\": \"premium\",\n",
        "            # \"sample_rate\": 48000,\n",
        "            \"speed\": adjust_speed\n",
        "        }\n",
        "        headers = {\n",
        "            \"accept\": \"text/event-stream\",\n",
        "            \"content-type\": \"application/json\",\n",
        "            \"AUTHORIZATION\": f\"{playHT_API_KEY}\",\n",
        "            \"X-USER-ID\": f\"{playHT_API_USER}\"\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, json=payload, headers=headers)\n",
        "        # Write the response text to the file\n",
        "        file.write(response.text + '\\n')\n",
        "\n",
        "print(f\"All responses have been saved to {base_directory}tts_responses_{video_sample}_adj_speed_{adjust_speed:.1f}.txt.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADOVkvYIHOPq"
      },
      "source": [
        "  ---\n",
        "  * Parse JSON from playHt to get audio url and duration\n",
        "\n",
        "** !!! Can not use duration from playHt response! Wrong number !!! **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAtPm3Z5PPEt"
      },
      "outputs": [],
      "source": [
        "# Path to the response file\n",
        "response_file = f\"{base_directory}tts_responses_{video_sample}_adj_speed_{adjust_speed:.1f}.txt\"\n",
        "\n",
        "# Initialize dictionaries to store the URLs and durations with labels\n",
        "audio_urls = {}\n",
        "audio_durations_ht = {}\n",
        "\n",
        "# Read and process the file\n",
        "with open(response_file, 'r') as file:\n",
        "    voice_clip_counter = 1  # Initialize counter for voice clips\n",
        "    for line in file:\n",
        "        if 'event: completed' in line:\n",
        "            # The next line contains the JSON data we need\n",
        "            data_line = next(file)\n",
        "            # Extract the part after 'data: '\n",
        "            json_str = data_line.split('data: ')[1].strip()\n",
        "            # Parse the JSON data\n",
        "            data = json.loads(json_str)\n",
        "            # Extract URL and duration\n",
        "            url = data['url']\n",
        "            duration = data['duration']\n",
        "            # Store data in dictionaries with labels\n",
        "            audio_urls[f\"VC{voice_clip_counter}\"] = url\n",
        "            audio_durations_ht[f\"VC{voice_clip_counter}\"] = duration\n",
        "            voice_clip_counter += 1\n",
        "\n",
        "# Output the results with labels\n",
        "print(\"Audio URLs:\")\n",
        "for label, url in audio_urls.items():\n",
        "    print(f\"{label}: {url}\")\n",
        "\n",
        "# print(\"\\nAudio Durations (seconds):\")\n",
        "# for label, duration in audio_durations_ht.items():\n",
        "#     print(f\"{label}: {duration} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrMVnfw8Hnv2"
      },
      "source": [
        "---\n",
        "# Download audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-QbzWNvAxTy"
      },
      "outputs": [],
      "source": [
        "# Function to download an audio file from a URL\n",
        "def download_audio(url, save_path):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
        "        with open(save_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Audio downloaded successfully: {save_path}\")\n",
        "        return True\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to download audio: {e}\")\n",
        "        return False\n",
        "\n",
        "# Function to calculate duration of an audio file\n",
        "def get_audio_duration(file_path):\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(file_path)\n",
        "        duration = len(audio) / 1000.0  # Convert from ms to seconds\n",
        "        return duration\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load audio for duration calculation: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Dictionary to store the durations\n",
        "audio_durations = {}\n",
        "combined_audio = AudioSegment.silent(duration=0)  # Initialize an empty audio segment\n",
        "\n",
        "# Loop through each audio URL entry and download it\n",
        "for label, url in audio_urls.items():\n",
        "    save_path = os.path.join(base_directory, f\"draft/{video_sample}_{label}.wav\")\n",
        "    if download_audio(url, save_path):\n",
        "        audio_segment = AudioSegment.from_file(save_path)\n",
        "        duration = len(audio_segment) / 1000.0\n",
        "        audio_durations[label] = duration\n",
        "        print(f\"Duration of {label}: {duration} seconds\")\n",
        "        combined_audio += audio_segment\n",
        "\n",
        "# Save the combined audio\n",
        "combined_save_path = os.path.join(base_directory, f\"draft/{video_sample}_adj_speed_{adjust_speed:.1f}_combined_audio_segments.wav\")\n",
        "combined_audio.export(combined_save_path, format=\"wav\")\n",
        "\n",
        "print(f\"Combined audio saved to: {combined_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuXUJ0mOH8Yq"
      },
      "source": [
        "---\n",
        "# 8. Duration alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j471MGSqyoob"
      },
      "outputs": [],
      "source": [
        "# Number of segments\n",
        "N = len(segment_durations)\n",
        "min_pause = 0.05\n",
        "\n",
        "# Sort the segments and VCs to ensure consistent ordering\n",
        "segment_keys = ['S{}'.format(i) for i in range(1, N+1)]\n",
        "vc_keys = ['VC{}'.format(i) for i in range(1, N+1)]\n",
        "pause_keys = ['P{}'.format(i) for i in range(1, N)]\n",
        "\n",
        "# Extract durations in order\n",
        "D_s = [segment_durations[key] for key in segment_keys]\n",
        "D_vc = [audio_durations[key] for key in vc_keys]\n",
        "D_p = [pauses_between_segments[key] for key in pause_keys]\n",
        "\n",
        "# Compute start times of original segments\n",
        "T_s = [0]  # Start time of first segment is 0\n",
        "# T_s = [float(first_segment_start_time)]\n",
        "for i in range(1, N):\n",
        "    T_s.append(T_s[i-1] + D_s[i-1] + D_p[i-1])\n",
        "\n",
        "# Compute midpoints of original segments\n",
        "M_s = [T_s[i] + D_s[i]/2 for i in range(N)]\n",
        "\n",
        "# Set up optimization variables\n",
        "T_vc = cp.Variable(N)  # Start times of synthesized VCs\n",
        "\n",
        "# Compute midpoints of synthesized VCs\n",
        "M_vc = T_vc + np.array(D_vc)/2\n",
        "\n",
        "# Compute pauses between synthesized VCs\n",
        "P_vc = T_vc[1:] - (T_vc[:-1] + D_vc[:-1])\n",
        "\n",
        "# Objective function: minimize sum of squared differences between midpoints and between pauses\n",
        "# Weight factor for pauses term\n",
        "\n",
        "w = 1\n",
        "\n",
        "objective = cp.Minimize(cp.sum_squares(M_s - M_vc) + w * cp.sum_squares(P_vc - D_p))\n",
        "\n",
        "# Constraints:\n",
        "constraints = []\n",
        "\n",
        "# Non-overlapping constraints\n",
        "for i in range(N-1):\n",
        "    constraints.append(T_vc[i+1] - (T_vc[i] + D_vc[i]) >= 0)\n",
        "\n",
        "# Start times non-negative\n",
        "constraints += [T_vc >= 0]\n",
        "constraints += [P_vc >= min_pause]\n",
        "\n",
        "# Form and solve the problem\n",
        "prob = cp.Problem(objective, constraints)\n",
        "prob.solve()\n",
        "\n",
        "# Print the results\n",
        "print(\"Optimal start times for synthesized VCs:\")\n",
        "for i in range(N):\n",
        "    print(f\"{vc_keys[i]}: {T_vc.value[i]:.6f}\")\n",
        "\n",
        "# Differences between midpoints after alignment\n",
        "print(\"\\nDifferences between midpoints after alignment:\")\n",
        "for i in range(N):\n",
        "    diff_mid = M_s[i] - (T_vc.value[i] + D_vc[i]/2)\n",
        "    print(f\"Segment {i+1} (S{i+1} vs {vc_keys[i]}): {diff_mid:.6f} seconds\")\n",
        "\n",
        "# Differences between pauses after alignment\n",
        "print(\"\\nDifferences between pauses after alignment:\")\n",
        "P_vc_values = T_vc.value[1:] - (T_vc.value[:-1] + D_vc[:-1])\n",
        "for i in range(N-1):\n",
        "    diff_pause = D_p[i] - P_vc_values[i]\n",
        "    print(f\"Pause {i+1} (P{i+1}): {diff_pause:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjIbGrvXAOp2"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "def analyze_alignment_quality(M_s, T_vc, D_vc, D_p, P_vc_values):\n",
        "    \"\"\"Analyze alignment quality between original segments and synthesized VCs.\"\"\"\n",
        "    M_vc = T_vc + np.array(D_vc)/2\n",
        "    midpoint_diffs = np.array(M_s) - M_vc\n",
        "    pause_diffs = np.array(D_p) - P_vc_values\n",
        "\n",
        "    stats_data = calculate_statistics(midpoint_diffs, pause_diffs, M_s, M_vc)\n",
        "    quality_assessment = assess_quality(stats_data)\n",
        "    create_visualizations(midpoint_diffs, pause_diffs, M_s, M_vc, quality_assessment)\n",
        "\n",
        "    return {**stats_data, 'quality_assessment': quality_assessment}\n",
        "\n",
        "def calculate_statistics(midpoint_diffs, pause_diffs, M_s, M_vc):\n",
        "    \"\"\"Calculate statistical measures for alignment analysis.\"\"\"\n",
        "    def get_stats(diffs):\n",
        "        return {\n",
        "            'mean_abs_error': np.mean(np.abs(diffs)),\n",
        "            'median_abs_error': np.median(np.abs(diffs)),\n",
        "            'std_dev': np.std(diffs),\n",
        "            'max_deviation': np.max(np.abs(diffs)),\n",
        "            'rmse': np.sqrt(np.mean(np.square(diffs))),\n",
        "            'percentiles': {\n",
        "                '90th': np.percentile(np.abs(diffs), 90),\n",
        "                '95th': np.percentile(np.abs(diffs), 95)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'midpoint_statistics': get_stats(midpoint_diffs),\n",
        "        'pause_statistics': get_stats(pause_diffs),\n",
        "        'timing_correlation': stats.pearsonr(M_s, M_vc)[0],\n",
        "        'total_alignment_error': np.sum(np.square(midpoint_diffs)) + np.sum(np.square(pause_diffs)),\n",
        "        'average_timing_deviation': np.mean(np.abs(midpoint_diffs)),\n",
        "        'significant_deviations': {\n",
        "            'midpoints': np.sum(np.abs(midpoint_diffs) > 0.5),\n",
        "            'pauses': np.sum(np.abs(pause_diffs) > 0.5)\n",
        "        }\n",
        "    }\n",
        "\n",
        "def assess_quality(stats_data):\n",
        "    \"\"\"Assess alignment quality based on thresholds.\"\"\"\n",
        "    thresholds = {\n",
        "        'midpoint': {'good': 0.2, 'acceptable': 0.5},\n",
        "        'pause': {'good': 0.3, 'acceptable': 0.7},\n",
        "        'correlation': {'good': 0.95, 'acceptable': 0.9},\n",
        "        'significant_deviations': {'good': 0.1, 'acceptable': 0.2}\n",
        "    }\n",
        "\n",
        "    quality_levels = {'good': 3, 'acceptable': 2, 'poor': 1}\n",
        "    assessment = {}\n",
        "\n",
        "    # Assess individual metrics\n",
        "    assessment['midpoint_quality'] = get_quality_level(\n",
        "        stats_data['midpoint_statistics']['mean_abs_error'],\n",
        "        thresholds['midpoint'])\n",
        "\n",
        "    assessment['pause_quality'] = get_quality_level(\n",
        "        stats_data['pause_statistics']['mean_abs_error'],\n",
        "        thresholds['pause'])\n",
        "\n",
        "    assessment['correlation_quality'] = get_quality_level(\n",
        "        stats_data['timing_correlation'],\n",
        "        thresholds['correlation'],\n",
        "        reverse=True)\n",
        "\n",
        "    total_deviations = sum(stats_data['significant_deviations'].values())\n",
        "    total_possible = stats_data['significant_deviations']['midpoints'] * 2 - 1\n",
        "    deviation_ratio = total_deviations / total_possible\n",
        "\n",
        "    assessment['deviation_quality'] = get_quality_level(\n",
        "        deviation_ratio,\n",
        "        thresholds['significant_deviations'])\n",
        "\n",
        "    # Calculate overall quality\n",
        "    total_score = sum(quality_levels[v] for v in assessment.values())\n",
        "    max_score = len(assessment) * 3\n",
        "\n",
        "    assessment['overall'] = (\n",
        "        'good' if total_score >= max_score * 0.8 else\n",
        "        'acceptable' if total_score >= max_score * 0.6 else\n",
        "        'poor'\n",
        "    )\n",
        "\n",
        "    return assessment\n",
        "\n",
        "def get_quality_level(value, thresholds, reverse=False):\n",
        "    \"\"\"Helper function to determine quality level.\"\"\"\n",
        "    if reverse:\n",
        "        return ('good' if value >= thresholds['good'] else\n",
        "                'acceptable' if value >= thresholds['acceptable'] else\n",
        "                'poor')\n",
        "    return ('good' if value <= thresholds['good'] else\n",
        "            'acceptable' if value <= thresholds['acceptable'] else\n",
        "            'poor')\n",
        "\n",
        "def create_visualizations(midpoint_diffs, pause_diffs, M_s, M_vc, quality_assessment):\n",
        "    \"\"\"Create visualization plots for alignment analysis.\"\"\"\n",
        "    plt.rcParams.update({'figure.figsize': [15, 10], 'axes.grid': True, 'grid.alpha': 0.3})\n",
        "    fig, axs = plt.subplots(2, 2)\n",
        "\n",
        "    def plot_distribution(ax, data, title, quality_key):\n",
        "        ax.hist(data, bins=20, density=True, alpha=0.7, color='blue')\n",
        "        kde = gaussian_kde(data)\n",
        "        x_range = np.linspace(min(data), max(data), 100)\n",
        "        ax.plot(x_range, kde(x_range), 'r-', lw=2)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('Time Difference (seconds)')\n",
        "        ax.set_ylabel('Density')\n",
        "        ax.text(0.05, 0.95, f\"Quality: {quality_assessment[quality_key]}\",\n",
        "                transform=ax.transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Plot distributions\n",
        "    plot_distribution(axs[0, 0], midpoint_diffs, 'Distribution of Midpoint Differences', 'midpoint_quality')\n",
        "    plot_distribution(axs[0, 1], pause_diffs, 'Distribution of Pause Differences', 'pause_quality')\n",
        "\n",
        "    # Scatter plot\n",
        "    axs[1, 0].scatter(M_s, M_vc, alpha=0.6, color='blue')\n",
        "    min_val, max_val = min(min(M_s), min(M_vc)), max(max(M_s), max(M_vc))\n",
        "    axs[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect alignment')\n",
        "    axs[1, 0].set_title('Original vs. Synthesized Midpoints')\n",
        "    axs[1, 0].set_xlabel('Original Midpoints (seconds)')\n",
        "    axs[1, 0].set_ylabel('Synthesized Midpoints (seconds)')\n",
        "    axs[1, 0].legend()\n",
        "    axs[1, 0].text(0.05, 0.95, f\"Quality: {quality_assessment['correlation_quality']}\",\n",
        "                   transform=axs[1, 0].transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Timeline plot\n",
        "    x_range = np.arange(len(midpoint_diffs))\n",
        "    axs[1, 1].plot(x_range, midpoint_diffs, 'b-', label='Midpoint differences', alpha=0.7)\n",
        "    axs[1, 1].plot(x_range[:-1], pause_diffs, 'g-', label='Pause differences', alpha=0.7)\n",
        "    axs[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "    axs[1, 1].set_title('Timeline of Timing Differences')\n",
        "    axs[1, 1].set_xlabel('Segment Number')\n",
        "    axs[1, 1].set_ylabel('Time Difference (seconds)')\n",
        "    axs[1, 1].legend()\n",
        "    axs[1, 1].text(0.05, 0.95, f\"Overall Quality: {quality_assessment['overall']}\",\n",
        "                   transform=axs[1, 1].transAxes, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_results(results):\n",
        "    \"\"\"Print detailed analysis results.\"\"\"\n",
        "    print(\"\\nAlignment Quality Analysis:\")\n",
        "\n",
        "    print(\"\\nMidpoint Statistics:\")\n",
        "    for key, value in results['midpoint_statistics'].items():\n",
        "        if key != 'percentiles':\n",
        "            print(f\"{key.replace('_', ' ').title()}: {value:.3f} seconds\")\n",
        "\n",
        "    print(\"\\nPause Statistics:\")\n",
        "    for key, value in results['pause_statistics'].items():\n",
        "        if key != 'percentiles':\n",
        "            print(f\"{key.replace('_', ' ').title()}: {value:.3f} seconds\")\n",
        "\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    print(f\"Timing Correlation: {results['timing_correlation']:.3f}\")\n",
        "    print(f\"Total Alignment Error: {results['total_alignment_error']:.3f}\")\n",
        "    print(f\"Average Timing Deviation: {results['average_timing_deviation']:.3f} seconds\")\n",
        "    print(f\"Number of Significant Midpoint Deviations (>0.5s): {results['significant_deviations']['midpoints']}\")\n",
        "    print(f\"Number of Significant Pause Deviations (>0.5s): {results['significant_deviations']['pauses']}\")\n",
        "\n",
        "    print(\"\\nQuality Assessment:\")\n",
        "    for aspect, quality in results['quality_assessment'].items():\n",
        "        print(f\"{aspect.replace('_', ' ').title()}: {quality}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6gCjFM_A2rO"
      },
      "outputs": [],
      "source": [
        "results = analyze_alignment_quality(M_s, T_vc.value, D_vc, D_p, P_vc_values)\n",
        "print_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdCADv1nG5vV"
      },
      "outputs": [],
      "source": [
        "P_vc_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9UGrsXBMEON"
      },
      "source": [
        "## Combine generated audio pieces together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6rYOBjI_MDWe"
      },
      "outputs": [],
      "source": [
        "# Ensure T_vc is a numpy array\n",
        "T_vc_values = T_vc.value.flatten()\n",
        "\n",
        "# Paths to the audio files\n",
        "N = len(T_vc_values)\n",
        "tag = \"_deepgram\"\n",
        "audio_files = [f\"{base_directory}draft/{video_sample}_VC{i}.wav\".format(i) for i in range(1, N+1)]\n",
        "original_audio_path = f\"{base_directory}{video_sample}.wav\"\n",
        "original_audio = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "\n",
        "# Load all audio segments\n",
        "vc_segments = []\n",
        "for i, file in enumerate(audio_files):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        audio = AudioSegment.from_wav(file)\n",
        "        vc_segments.append(audio)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Audio file {file} not found.\")\n",
        "        exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apii0RcFI6_o"
      },
      "outputs": [],
      "source": [
        "# Initialize the combined audio with the first segment\n",
        "combined_audio = vc_segments[0]\n",
        "current_time = T_vc_values[0]\n",
        "\n",
        "for i in range(1, N):\n",
        "    prev_segment = vc_segments[i - 1]\n",
        "    current_segment = vc_segments[i]\n",
        "\n",
        "    # Calculate the expected start time of the current segment\n",
        "    expected_start_time = T_vc_values[i]\n",
        "    prev_end_time = T_vc_values[i - 1] + D_vc[i - 1]\n",
        "\n",
        "    # Time difference in milliseconds\n",
        "    time_diff_ms = int((expected_start_time - prev_end_time) * 1000)\n",
        "\n",
        "    # Crossfade duration in milliseconds\n",
        "    fade_duration_ms = min_pause * 1000\n",
        "\n",
        "    # Ensure crossfade duration does not exceed half the duration of either segment\n",
        "    fade_duration_ms = int(min(fade_duration_ms,\n",
        "                                    prev_segment.duration_seconds * 1000 / 2,\n",
        "                                    current_segment.duration_seconds * 1000 / 2))\n",
        "\n",
        "    # print(f\"time_diff_ms: {time_diff_ms}, fade_duration_ms: {fade_duration_ms}\")\n",
        "    # print(prev_segment.duration_seconds,current_segment.duration_seconds)\n",
        "\n",
        "    if time_diff_ms > 0:\n",
        "        if len(current_segment) > 1:\n",
        "            current_segment = current_segment.fade_in(duration = fade_duration_ms)\n",
        "        if len(prev_segment) > 1:\n",
        "            combined_audio = combined_audio.fade_out(duration = fade_duration_ms)\n",
        "\n",
        "        # Combine the segments with crossfade\n",
        "        # Add silence if there's a gap\n",
        "        silence = AudioSegment.silent(duration=time_diff_ms)\n",
        "        combined_audio = combined_audio + silence + current_segment\n",
        "\n",
        "        # Append the current segment without crossfade\n",
        "        # combined_audio += current_segment\n",
        "    else:\n",
        "        print(\"!!! Raise warning !!!\")\n",
        "\n",
        "    current_time = expected_start_time\n",
        "\n",
        "start_pause = 0\n",
        "if first_segment_start_time - T_vc_values[0] > 0:\n",
        "    start_pause = first_segment_start_time-T_vc_values[0]\n",
        "else:\n",
        "    start_pause = 0\n",
        "\n",
        "combined_audio = AudioSegment.silent(duration=start_pause * 1000) + combined_audio.fade_in(duration = min_pause * 1000)\n",
        "\n",
        "end_pause = 0\n",
        "if len(original_audio) - len(combined_audio) > 0:\n",
        "    end_pause = (len(original_audio) - len(combined_audio)) /1000\n",
        "else:\n",
        "    end_pause = 0\n",
        "\n",
        "combined_audio = combined_audio.fade_out(duration = min_pause * 1000) + AudioSegment.silent(duration=end_pause * 1000)\n",
        "print(f\"start_pause:{start_pause}, end_pause:{end_pause}, combined_audio:{len(combined_audio)/1000}\")\n",
        "\n",
        "# Export the combined audio\n",
        "combined_audio.export(f\"{base_directory}draft/{video_sample}_adj_speed_{adjust_speed:.1f}_combined_audio.wav\", format='wav')\n",
        "\n",
        "print(\"Combined audio saved as 'combined_audio.wav'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3B7iRhuxF1I"
      },
      "outputs": [],
      "source": [
        "print(f'first_original_segment_start_time: {first_segment_start_time:.4f}\\nfirst_vc_segment_start: {T_vc_values[0]:.4f}\\ndifference: {first_segment_start_time-T_vc_values[0]:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPB1GBMGQYx1"
      },
      "outputs": [],
      "source": [
        "original_audio_path = f\"{base_directory}{video_sample}.wav\"\n",
        "original_audio = AudioSegment.from_file(original_audio_path)\n",
        "\n",
        "original_duration = last_segment_end_time - first_segment_start_time\n",
        "print(f\"(len(original_audio) / 1000.0):{(len(original_audio) / 1000.0)}, original_segment_duration:{original_duration}\")\n",
        "print(f\"first_segment_start_time:{first_segment_start_time}, last_segment_end_time:{last_segment_end_time}\")\n",
        "print(f\"combined_audio.duration_seconds:{combined_audio.duration_seconds}\")\n",
        "print(len(combined_audio)/1000 + first_segment_start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYU4G6MFPcHj"
      },
      "source": [
        "# 9. Replace the polished audio to the original video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKx7InpPczQC"
      },
      "outputs": [],
      "source": [
        "video_filename = f\"/content/drive/MyDrive/{video_sample}_original.mp4\"\n",
        "\n",
        "# Load the video file\n",
        "video_clip = VideoFileClip(video_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYpxXCLdAT-i"
      },
      "outputs": [],
      "source": [
        "new_audio_clip = AudioFileClip(f\"{base_directory}draft/{video_sample}_adj_speed_{adjust_speed:.1f}_combined_audio.wav\")\n",
        "\n",
        "# Set the audio of the cut video clip to the new audio clip\n",
        "final_video_clip = video_clip.set_audio(new_audio_clip)\n",
        "\n",
        "# Save the final video with the new audio\n",
        "final_video_clip.write_videofile(f\"{base_directory}Polished_{video_sample}_speed_{adjust_speed}_global.mp4\", codec=\"libx264\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mtJuupHRiKG"
      },
      "outputs": [],
      "source": [
        "# Close the clips to release resources\n",
        "new_audio_clip.close()\n",
        "final_video_clip.close()\n",
        "video_clip.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
